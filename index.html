<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Ashwin Khadke</title>
  
  <meta name="author" content="Ashwin Rajendra Khadke">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Ashwin Khadke</name>
              </p>
              <p>I am a Ph.D. candidate at <a href="https://ri.cmu.edu/"> Robotics Institute, Carnegie Mellon University</a>, where I work on control theory and Reinforcement Learning. I am advised by <a href="https://www.cs.cmu.edu/~hgeyer/">Prof. Hartmut Geyer</a>
              </p>
              <p>
                I work on approximately decomposing high-dimensional systems into easily solvable dynamical systems. 
              </p>
              <p style="text-align:center">
                <a href="mailto:akhadke@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="data/CV.pdf">CV</a> &nbsp/&nbsp -->
                <a href="data/my-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=amQYGHEAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/ash22194">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/ashwin_circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/ashwin_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in control theory, Reinforcement Learning and Advanced Multi-Body Simulation.  I, specifically, interested in applying control techniques to high-dimensional bipedal systems.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <!-- project entry -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/project_policydecomposition.png" alt="policydecomposition" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/9555796">
                <papertitle>Policy Decomposition: Approximate Optimal Control with Suboptimality Estimates</papertitle>
              </a>
              <br>
              <strong>Ashwin Khadke</strong>,
              <a href="https://www.cs.cmu.edu/~hgeyer/">Hartmut Geyer</a>
              <br>
              <em>Humanoids</em>, 2020
              <br>
              <a href="data/policydecomposition.bib">bibtex</a>
              <p>
                Numerically computing global policies to optimal control problems for complex dynamical systems is mostly in- tractable. In consequence, a number of approximation methods have been developed. However, none of the current methods can quantify by how much the resulting control underperforms the elusive globally optimal solution. Here we propose policy decomposition, an approximation method with explicit subopti- mality estimates. Our method decomposes the optimal control problem into lower-dimensional subproblems, whose optimal solutions are recombined to build a control policy for the entire system. Many such combinations exist, and we introduce the value error and its LQR and DDP estimates to predict the suboptimality of possible combinations and prioritize the ones that minimize it. Using a cart-pole, a 3-link balancing biped and N-link planar manipulators as example systems, we find that the estimates correctly identify the best combinations, yielding control policies in a fraction of the time it takes to compute the optimal control without a notable sacrifice in closed-loop performance. While more research will be needed to find ways of dealing with the combinatorics of policy decomposition, the results suggest this method could be an effective alternative for approximating optimal control in intractable systems.

                </p>
              </td>
          </tr>

          <!-- project entry -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/project_workshopakshara.png" alt="project1" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openreview.net/forum?id=XcgrVrW1BSm">
                <papertitle>Policy Iteration with Gaussian Process based Value Function Approximation</papertitle>
              </a>
              <br>
              <strong>Ashwin Khadke</strong>,
              <a href="https://ai.facebook.com/people/akshara-rai/">Akshara Rai</a>
              <br>
              <em>Robotics Retrospectives Workshop</em>, 2020
              <br>
              <a href="https://drive.google.com/file/d/0B4nuwEMaEsnmLUQ5SVJTcUZIYXc/view?usp=sharing">supplement</a> / <a href="data/KarschCVPR2013.bib">bibtex</a>
              <p>In this work, we explore the use of Gaussian processes (GP) as function approximators for Reinforcement Learning (RL), and build estimates of the value function and Q-function using GPs. Such a representation allows us to learn Q-functions, and thereby policies, conditioned on uncertainty in the system dynamics, and can be useful in sample efficiently transferring policies learned in simulation to hardware.

                We use two approaches, GPTD and GPSARSA, to build approximate value functions and Q-functions respectively. While for simple, continuous problems, we found these to be effective at approximating the value function and the Q-function, for discontinuous landscapes GPSARSA deteriorates in performance, even on simple problems. As the problem complexity increases, for example, for an inverted pendulum, we find that both approaches are extremely sensitive to the GP hyperparameters, and do not scale well. We experiment with a sparse variant of the algorithm but find that GPSARSA still converges to poor solutions. Our experiments show that while GPTD and GPSARSA are nice theoretical formulations, they are not suitable for complex domains without extensive hyperparameter tuning.
              </p>
                
              </td>
          </tr>

          <!-- project 2 -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/project_ishani.png" alt="project2" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ojs.aaai.org//index.php/ICAPS/article/view/3535/3403">
                <papertitle>Speeding Up Search-Based Motion Planning via Conservative Heuristics</papertitle>
              </a>
              <br>
              <a href="https://www.ri.cmu.edu/ri-people/ishani-chatterjee/">Ishani Chatterjee</a>,
              <strong>Ashwin Khadke</strong>,
              <a href="http://www.cs.cmu.edu/~maxim/">Maxim Likhachev</a>,
              <a href="http://www.cs.cmu.edu/~mmv/">Manuela Veloso</a>
              <br>
              <em>ICAPS</em>, 2019
              <br>
                <a href="data/ishaniicaps19.bib">bibtex</a>
              <p>
                Weighted A* search (wA*) is a popular tool for robot motion-planning. Its efficiency however depends on the quality of heuristic function used. In fact, it has been shown that the correlation between the heuristic function and the true cost-to-goal significantly affects the efficiency of the search, when used with a large weight on the heuristics. Motivated by this observation, we investigate the problem of computing heuristics that explicitly aim to minimize the amount of search efforts in finding a feasible plan. The key observation we exploit is that while heuristics tries to guide the search along what looks like an optimal path towards the goal, there are other paths that are clearly sub-optimal yet are much easier to compute. For example, in motion planning domains like footstep-planning for humanoids, a heuristic that guides the search along a path away from obstacles is less likely to encounter local minima compared with the heuristics that guides the search along an optimal but close-to-obstacles path. We utilize this observation to define the concept of conservative heuristics and propose a simple algorithm for computing such a heuristic function. Experimental analysis on (1) humanoid footstep planning (simulation), (2) path planning for a UAV (simulation), and a real-world experiment in footstep-planning for a NAO robot shows the utility of the approach.
                </p>

              </td>
          </tr>

          <!-- project 3 -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/project_expertexploration.jpg" alt="project3" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.ri.cmu.edu/publications/exploration-with-expert-policy-advice/">
                <papertitle>Exploration with Expert Policy Advice</papertitle>
              </a>
              <br>
              <strong>Ashwin Khadke</strong>,
              <a href="https://www.andrew.cmu.edu/user/arpita1/">Arpit Agarwal</a>,
              <a href="https://scholar.google.com/citations?user=F2inK2UAAAAJ&hl=en"Anahita Mohseni Kabir</a>,
              <a href="https://www.ri.cmu.edu/ri-people/devin-schwab/">Devin Schwab</a>
              <br>
              <em>Technical Report</em>, 2018
              <br>
                <a href="data/courseproject.bib">bibtex</a>
              <p>
                Exploration for Reinforcement Learning is a challenging problem. Random exploration is often highly inefficient and in sparse reward environments may completely fail. In this work, we developed a novel method which incorporates ex- pert advice for exploration in sparse reward environments. In our formulation, the agent has access to a set of expert policies and learns to bias its exploration based on the ex- perts‚Äô suggested actions. By incorporating expert suggestions the agent is able to quickly learn a policy to reach rewarding states. Our method can mix and match experts‚Äô advice during an episode to reach goal states. Moreover, our formulation does not restrict the agent to any policy set. This allows us to aim for a globally optimal solution. In our experiments, we show that using expert advice indeed leads to faster explo- ration in challenging grid-world environments.
                </p>

              </td>
          </tr>

          <!-- project 4 -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mastersthesis.png" alt="project4" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.ri.cmu.edu/publications/what-can-this-robot-do-learning-capability-models-from-appearance-and-experiments/">
                <papertitle>What Can This Robot Do? Learning Capability Models from Appearance and Experiments</papertitle>
              </a>
              <br>
              <strong>Ashwin Khadke</strong>
              <em>Master's Thesis, CMU</em>, 2018
              <br>
                <a href="data/masters.bib">bibtex</a>
              <p>

As autonomous robots become increasingly multifunctional and adaptive, it becomes difficult to determine the extent of their capabilities, i.e. the tasks they can perform and their strengths and limitations at these tasks. A robot‚Äôs appearance can provide cues to its physical as well as cognitive capabilities. We present an algorithm that builds on these cues and learns models of a robot‚Äôs ability to perform different tasks through active experimentation. These models not only capture the robot‚Äôs inherent abilities but also incorporate the effect of relevant extrinsic factors on a robot‚Äôs performance. Our algorithm would find use as a tool for humans in determining ‚ÄùWhat can this robot do?‚Äù.
We applied our algorithm in modelling a NAO and a Pepper robot at two different tasks. We first illustrate the advantages of our active experimen- tation approach over building models through passive observations. Next, we show the utility of such models in identifying scenarios a robot is well suited for in performing a task. Finally, we demonstrate the use of such models in a collaborative human-robot task.
                </p>

              </td>
          </tr>

          <!-- project 5 -->
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/mastersconf1.png" alt="project5" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://link.springer.com/chapter/10.1007/978-3-030-01370-7_69">
                <papertitle>What Can This Robot Do? Learning Capability Models from Appearance and Experiments</papertitle>
              </a>
              <br>
              <strong>Ashwin Khadke</strong>
              <em>IAS</em>, 2018
              <br>
                <a href="data/mastersconf1.bib">bibtex</a>
              <p>

As autonomous robots become increasingly multifunctional and adaptive, it becomes difficult to determine the extent of their capabilities, i.e. the tasks they can perform and their strengths and limitations at these tasks. A robot‚Äôs appearance can provide cues to its physical as well as cognitive capabilities. We present an algorithm that builds on these cues and learns models of a robot‚Äôs ability to perform different tasks through active experimentation. These models not only capture the robot‚Äôs inherent abilities but also incorporate the effect of relevant extrinsic factors on a robot‚Äôs performance. Our algorithm would find use as a tool for humans in determining ‚ÄùWhat can this robot do?‚Äù.
We applied our algorithm in modelling a NAO and a Pepper robot at two different tasks. We first illustrate the advantages of our active experimen- tation approach over building models through passive observations. Next, we show the utility of such models in identifying scenarios a robot is well suited for in performing a task. Finally, we demonstrate the use of such models in a collaborative human-robot task.
                </p>

              </td>
          </tr>


        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Misc</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <!-- <tr> -->
            <!-- <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td> -->
            <!-- <td width="75%" valign="center"> -->
              <!-- <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
              <br> -->
            <!-- </td>
          </tr> -->
          <tr>
            <!-- <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/cs188.jpg" alt="cs188">
            </td> -->
            <td width="100%" valign="center"> 
              <a href="https://www.cs.cmu.edu/~hgeyer/Teaching_16-711.html">Graduate Student Instructor, 16-711 Fall 2021</a>
              <br>
              <a href="https://sites.google.com/site/robotkinematicscmu/">Graduate Student Instructor, 16-384 Fall 2020</a>
            </td>
          </tr>
					


					
					
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:left;font-size:small;">
                Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>,
                just add a link back to my website.
                <br>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
